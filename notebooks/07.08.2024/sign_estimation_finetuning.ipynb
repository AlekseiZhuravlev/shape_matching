{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import trimesh\n",
    "\n",
    "scene = trimesh.Scene()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading base dataset: 100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from my_code.datasets.surreal_dataset_3dc import TemplateSurrealDataset3DC\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import my_code.diffusion_training_sign_corr.data_loading as data_loading\n",
    "\n",
    "\n",
    "train_dataset = data_loading.get_val_dataset(\n",
    "    'FAUST_a', 'train', 128, preload=True, canonicalize_fmap=None\n",
    "    )[1]\n",
    "\n",
    "test_datasets = {\n",
    "    'FAUST_a': data_loading.get_val_dataset(\n",
    "        'FAUST_a', 'train', 128, preload=False, canonicalize_fmap=None\n",
    "        )[1],\n",
    "    'FAUST_orig train': data_loading.get_val_dataset(\n",
    "        'FAUST_orig', 'train', 128, preload=False, canonicalize_fmap=None\n",
    "        )[1],\n",
    "    'FAUST_orig test': data_loading.get_val_dataset(\n",
    "        'FAUST_orig', 'test', 128, preload=False, canonicalize_fmap=None\n",
    "        )[1],\n",
    "    'FAUST_r train': data_loading.get_val_dataset(\n",
    "        'FAUST_r', 'train', 128, preload=False, canonicalize_fmap=None\n",
    "        )[1],\n",
    "    'FAUST_r test': data_loading.get_val_dataset(\n",
    "        'FAUST_r', 'test', 128, preload=False, canonicalize_fmap=None\n",
    "        )[1]\n",
    "}\n",
    "\n",
    "# test_dataset = data_loading.get_val_dataset(\n",
    "#     'FAUST_orig', 'test', 200, canonicalize_fmap=None\n",
    "# )[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks.diffusion_network as diffusion_network\n",
    "\n",
    "condition_dim = 0\n",
    "start_dim = 0\n",
    "\n",
    "feature_dim = 32\n",
    "evecs_per_support = 4\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = diffusion_network.DiffusionNet(\n",
    "    in_channels=feature_dim,\n",
    "    out_channels=feature_dim // evecs_per_support,\n",
    "    cache_dir=None,\n",
    "    input_type='wks',\n",
    "    k_eig=128,\n",
    "    n_block=6\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('/home/s94zalek_hpc/shape_matching/my_code/experiments/sign_double_start_0_feat_32_6block_factor4_dataset_SURREAL_train_rot_180_180_180_normal_True_noise_0.0_-0.05_0.05_lapl_mesh_scale_0.9_1.1_wks/40000.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import my_code.sign_canonicalization.training as sign_training\n",
    "\n",
    "\n",
    "def test_sign_correction(net, test_dataset):\n",
    "    \n",
    "    tqdm._instances.clear()\n",
    "\n",
    "    n_epochs = 5\n",
    "        \n",
    "    # iterator = tqdm(total=len(test_dataset) * n_epochs)\n",
    "    incorrect_signs_list = torch.tensor([])\n",
    "    curr_iter = 0\n",
    "\n",
    "        \n",
    "    for _ in range(n_epochs):\n",
    "        for curr_idx in range(len(test_dataset)):\n",
    "\n",
    "            ##############################################\n",
    "            # Select a shape\n",
    "            ##############################################\n",
    "\n",
    "            train_shape = test_dataset[curr_idx]['second']\n",
    "\n",
    "            # train_shape = double_shape['second']\n",
    "            verts = train_shape['verts'].unsqueeze(0).to(device)\n",
    "            faces = train_shape['faces'].unsqueeze(0).to(device)    \n",
    "\n",
    "            evecs_orig = train_shape['evecs'].unsqueeze(0)[:, :, start_dim:start_dim+feature_dim].to(device)\n",
    "            \n",
    "            mass_mat = torch.diag_embed(\n",
    "                torch.ones_like(train_shape['mass'].unsqueeze(0))\n",
    "                ).to(device)\n",
    "\n",
    "            ##############################################\n",
    "            # Set the signs on shape 0\n",
    "            ##############################################\n",
    "\n",
    "            # create a random combilation of +1 and -1, length = feature_dim\n",
    "            sign_gt_0 = torch.randint(0, 2, (feature_dim,)).float().to(device)\n",
    "            \n",
    "            sign_gt_0[sign_gt_0 == 0] = -1\n",
    "            sign_gt_0 = sign_gt_0.float().unsqueeze(0)\n",
    "\n",
    "            # multiply evecs [6890 x 16] by sign_flip [16]\n",
    "            evecs_flip_0 = evecs_orig * sign_gt_0\n",
    "            \n",
    "            # predict the sign change\n",
    "            with torch.no_grad():\n",
    "                sign_pred_0, supp_vec_0, _ = sign_training.predict_sign_change(\n",
    "                    net, verts, faces, evecs_flip_0, \n",
    "                    mass_mat=mass_mat, input_type=net.input_type,\n",
    "                    \n",
    "                    mass=train_shape['mass'].unsqueeze(0), L=train_shape['L'].unsqueeze(0),\n",
    "                    evals=train_shape['evals'].unsqueeze(0), evecs=train_shape['evecs'].unsqueeze(0),\n",
    "                    gradX=train_shape['gradX'].unsqueeze(0), gradY=train_shape['gradY'].unsqueeze(0)\n",
    "                    )\n",
    "            \n",
    "            ##############################################\n",
    "            # Set the signs on shape 1\n",
    "            ##############################################\n",
    "            \n",
    "            # create a random combilation of +1 and -1, length = feature_dim\n",
    "            sign_gt_1 = torch.randint(0, 2, (feature_dim,)).float().to(device)\n",
    "            \n",
    "            sign_gt_1[sign_gt_1 == 0] = -1\n",
    "            sign_gt_1 = sign_gt_1.float().unsqueeze(0)\n",
    "            \n",
    "            # multiply evecs [6890 x 16] by sign_flip [16]\n",
    "            evecs_flip_1 = evecs_orig * sign_gt_1\n",
    "            \n",
    "            # predict the sign change\n",
    "            with torch.no_grad():\n",
    "                sign_pred_1, supp_vec_1, _ = sign_training.predict_sign_change(\n",
    "                    net, verts, faces, evecs_flip_1, \n",
    "                    mass_mat=mass_mat, input_type=net.input_type,\n",
    "                    \n",
    "                    mass=train_shape['mass'].unsqueeze(0), L=train_shape['L'].unsqueeze(0),\n",
    "                    evals=train_shape['evals'].unsqueeze(0), evecs=train_shape['evecs'].unsqueeze(0),\n",
    "                    gradX=train_shape['gradX'].unsqueeze(0), gradY=train_shape['gradY'].unsqueeze(0)\n",
    "                    )\n",
    "            \n",
    "            ##############################################\n",
    "            # Calculate the loss\n",
    "            ##############################################\n",
    "            \n",
    "            # calculate the ground truth sign difference\n",
    "            sign_diff_gt = sign_gt_1 * sign_gt_0\n",
    "            \n",
    "            # calculate the sign difference between predicted evecs\n",
    "            sign_diff_pred = sign_pred_1 * sign_pred_0\n",
    "            \n",
    "            sign_correct = sign_diff_pred.sign() * sign_diff_gt.sign() \n",
    "            \n",
    "            \n",
    "            # count the number of incorrect signs\n",
    "            count_incorrect_signs = (sign_correct < 0).int().sum()\n",
    "                \n",
    "            # incorrect_signs_list.append(count_incorrect_signs)\n",
    "            incorrect_signs_list = torch.cat([incorrect_signs_list, torch.tensor([count_incorrect_signs])])\n",
    "            \n",
    "            \n",
    "            # iterator.set_description(f'Mean incorrect signs {incorrect_signs_list.float().mean():.2f} / {feature_dim}, max {incorrect_signs_list.max()}')\n",
    "            # iterator.update(1)\n",
    "            # if count_incorrect_signs > 7:\n",
    "            #     raise ValueError('Too many incorrect signs')\n",
    "        \n",
    "    return incorrect_signs_list.float().mean(), incorrect_signs_list.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "    opt, start_factor=1, end_factor=0.1, \n",
    "    total_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "FAUST_a: mean = 1.17 / 32, max = 5\n",
      "FAUST_orig train: mean = 0.25 / 32, max = 5\n",
      "FAUST_orig test: mean = 0.42 / 32, max = 3\n",
      "FAUST_r train: mean = 0.59 / 32, max = 6\n",
      "FAUST_r test: mean = 0.89 / 32, max = 9\n",
      "100\n",
      "FAUST_a: mean = 0.22 / 32, max = 3\n",
      "FAUST_orig train: mean = 0.63 / 32, max = 5\n",
      "FAUST_orig test: mean = 0.70 / 32, max = 5\n",
      "FAUST_r train: mean = 0.83 / 32, max = 7\n",
      "FAUST_r test: mean = 0.99 / 32, max = 6\n",
      "200\n",
      "FAUST_a: mean = 0.09 / 32, max = 2\n",
      "FAUST_orig train: mean = 0.64 / 32, max = 7\n",
      "FAUST_orig test: mean = 0.65 / 32, max = 3\n",
      "FAUST_r train: mean = 0.97 / 32, max = 8\n",
      "FAUST_r test: mean = 1.01 / 32, max = 9\n",
      "300\n",
      "FAUST_a: mean = 0.21 / 32, max = 2\n",
      "FAUST_orig train: mean = 0.61 / 32, max = 5\n",
      "FAUST_orig test: mean = 0.62 / 32, max = 7\n",
      "FAUST_r train: mean = 0.72 / 32, max = 7\n",
      "FAUST_r test: mean = 0.91 / 32, max = 6\n",
      "400\n",
      "FAUST_a: mean = 0.07 / 32, max = 1\n",
      "FAUST_orig train: mean = 0.62 / 32, max = 5\n",
      "FAUST_orig test: mean = 0.73 / 32, max = 4\n",
      "FAUST_r train: mean = 0.70 / 32, max = 6\n",
      "FAUST_r test: mean = 0.60 / 32, max = 4\n",
      "500\n",
      "FAUST_a: mean = 0.02 / 32, max = 1\n",
      "FAUST_orig train: mean = 0.66 / 32, max = 5\n",
      "FAUST_orig test: mean = 0.48 / 32, max = 3\n",
      "FAUST_r train: mean = 0.80 / 32, max = 7\n",
      "FAUST_r test: mean = 0.60 / 32, max = 4\n",
      "600\n",
      "FAUST_a: mean = 0.14 / 32, max = 2\n",
      "FAUST_orig train: mean = 0.77 / 32, max = 7\n",
      "FAUST_orig test: mean = 0.66 / 32, max = 4\n",
      "FAUST_r train: mean = 0.81 / 32, max = 7\n",
      "FAUST_r test: mean = 0.70 / 32, max = 5\n",
      "700\n",
      "FAUST_a: mean = 0.12 / 32, max = 2\n",
      "FAUST_orig train: mean = 0.72 / 32, max = 5\n",
      "FAUST_orig test: mean = 0.71 / 32, max = 3\n",
      "FAUST_r train: mean = 0.79 / 32, max = 5\n",
      "FAUST_r test: mean = 0.63 / 32, max = 7\n",
      "800\n",
      "FAUST_a: mean = 0.13 / 32, max = 2\n",
      "FAUST_orig train: mean = 0.62 / 32, max = 7\n",
      "FAUST_orig test: mean = 0.64 / 32, max = 7\n",
      "FAUST_r train: mean = 0.72 / 32, max = 8\n",
      "FAUST_r test: mean = 0.59 / 32, max = 3\n",
      "900\n",
      "FAUST_a: mean = 0.07 / 32, max = 1\n",
      "FAUST_orig train: mean = 0.75 / 32, max = 6\n",
      "FAUST_orig test: mean = 0.63 / 32, max = 3\n",
      "FAUST_r train: mean = 0.84 / 32, max = 6\n",
      "FAUST_r test: mean = 0.68 / 32, max = 3\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import my_code.sign_canonicalization.training as sign_training\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "tqdm._instances.clear()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "losses = torch.tensor([])\n",
    "# train_iterator = tqdm(range(1000))\n",
    "train_iterator = range(1000)     \n",
    "        \n",
    "curr_iter = 0\n",
    "for epoch in range(len(train_iterator) // len(train_dataset)):\n",
    "    \n",
    "    # train_shapes_shuffled = train_shapes.copy()\n",
    "    # np.random.shuffle(train_shapes)\n",
    "    \n",
    "    \n",
    "    for curr_idx in range(len(train_dataset)):\n",
    "\n",
    "        ##############################################\n",
    "        # Select a shape\n",
    "        ##############################################\n",
    "        # curr_idx = np.random.randint(0, len(train_shapes))\n",
    "    \n",
    "        train_shape = train_dataset[curr_idx]['second']\n",
    "\n",
    "        # train_shape = double_shape['second']\n",
    "        verts = train_shape['verts'].unsqueeze(0).to(device)\n",
    "        faces = train_shape['faces'].unsqueeze(0).to(device)    \n",
    "\n",
    "        evecs_orig = train_shape['evecs'].unsqueeze(0)[:, :, start_dim:start_dim+feature_dim].to(device)\n",
    "        \n",
    "        mass_mat = torch.diag_embed(\n",
    "            torch.ones_like(train_shape['mass'].unsqueeze(0))\n",
    "            ).to(device)\n",
    "\n",
    "        ##############################################\n",
    "        # Set the signs on shape 0\n",
    "        ##############################################\n",
    "\n",
    "        # create a random combilation of +1 and -1, length = feature_dim\n",
    "        sign_gt_0 = torch.randint(0, 2, (feature_dim,)).float().to(device)\n",
    "        \n",
    "        sign_gt_0[sign_gt_0 == 0] = -1\n",
    "        sign_gt_0 = sign_gt_0.float().unsqueeze(0)\n",
    "\n",
    "        # multiply evecs [6890 x 16] by sign_flip [16]\n",
    "        evecs_flip_0 = evecs_orig * sign_gt_0\n",
    "        \n",
    "        # predict the sign change\n",
    "        sign_pred_0 = sign_training.predict_sign_change(\n",
    "            net, verts, faces, evecs_flip_0, \n",
    "            mass_mat=mass_mat, input_type=net.input_type,\n",
    "            \n",
    "            mass=train_shape['mass'].unsqueeze(0), L=train_shape['L'].unsqueeze(0),\n",
    "            evals=train_shape['evals'].unsqueeze(0), evecs=train_shape['evecs'].unsqueeze(0),\n",
    "            gradX=train_shape['gradX'].unsqueeze(0), gradY=train_shape['gradY'].unsqueeze(0)\n",
    "            )[0]\n",
    "        \n",
    "        ##############################################\n",
    "        # Set the signs on shape 1\n",
    "        ##############################################\n",
    "        \n",
    "        # create a random combilation of +1 and -1, length = feature_dim\n",
    "        sign_gt_1 = torch.randint(0, 2, (feature_dim,)).float().to(device)\n",
    "        \n",
    "        sign_gt_1[sign_gt_1 == 0] = -1\n",
    "        sign_gt_1 = sign_gt_1.float().unsqueeze(0)\n",
    "        \n",
    "        # multiply evecs [6890 x 16] by sign_flip [16]\n",
    "        evecs_flip_1 = evecs_orig * sign_gt_1\n",
    "        \n",
    "        # predict the sign change\n",
    "        sign_pred_1 = sign_training.predict_sign_change(\n",
    "            net, verts, faces, evecs_flip_1, \n",
    "            mass_mat=mass_mat, input_type=net.input_type,\n",
    "            \n",
    "            mass=train_shape['mass'].unsqueeze(0), L=train_shape['L'].unsqueeze(0),\n",
    "            evals=train_shape['evals'].unsqueeze(0), evecs=train_shape['evecs'].unsqueeze(0),\n",
    "            gradX=train_shape['gradX'].unsqueeze(0), gradY=train_shape['gradY'].unsqueeze(0)\n",
    "            )[0]\n",
    "        \n",
    "        ##############################################\n",
    "        # Calculate the loss\n",
    "        ##############################################\n",
    "        \n",
    "        # calculate the ground truth sign difference\n",
    "        sign_diff_gt = sign_gt_1 * sign_gt_0\n",
    "        \n",
    "        # calculate the sign difference between predicted evecs\n",
    "        sign_diff_pred = sign_pred_1 * sign_pred_0\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = loss_fn(\n",
    "            sign_diff_pred.reshape(sign_diff_pred.shape[0], -1),\n",
    "            sign_diff_gt.reshape(sign_diff_gt.shape[0], -1)\n",
    "            )\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses = torch.cat([losses, torch.tensor([loss.item()])])\n",
    "        \n",
    "        # print mean of last 10 losses\n",
    "        # train_iterator.set_description(f'loss={torch.mean(losses[-10:]):.3f}')\n",
    "        \n",
    "        # plot the losses every 1000 iterations\n",
    "        if curr_iter == 0 or curr_iter % (len(train_iterator) // 10) == 0:\n",
    "            \n",
    "            print(f'{curr_iter}')\n",
    "            \n",
    "            for test_dataset_name in test_datasets.keys():\n",
    "                mean_incorrect, max_incorrect = test_sign_correction(net, test_datasets[test_dataset_name])\n",
    "                \n",
    "                print(f'{test_dataset_name}: mean = {mean_incorrect:.2f} / {feature_dim}, max = {int(max_incorrect)}')\n",
    "            \n",
    "            # mean_incorrect, max_incorrect = test_sign_correction(net, train_dataset)\n",
    "            \n",
    "            # print(f'{curr_iter}: mean = {mean_incorrect:.2f} / {feature_dim}, max = {int(max_incorrect)}')\n",
    "            \n",
    "        curr_iter += 1\n",
    "        # train_iterator.update(1)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
