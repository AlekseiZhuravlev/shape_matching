{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networks.diffusion_network as diffusion_network\n",
    "from tqdm import tqdm\n",
    "import my_code.sign_canonicalization.training as sign_training\n",
    "import my_code.sign_canonicalization.remesh as remesh\n",
    "import torch\n",
    "import my_code.diffusion_training_sign_corr.data_loading as data_loading\n",
    "import yaml\n",
    "import my_code.datasets.preprocessing as preprocessing\n",
    "import trimesh\n",
    "import argparse\n",
    "import utils.fmap_util as fmap_util\n",
    "import numpy as np\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'signNet_remeshed_mass_6b_1ev_10_0.2_0.8'\n",
    "\n",
    "exp_dir = f'/home/s94zalek_hpc/shape_matching/my_code/experiments/sign_net/{exp_name}'\n",
    "\n",
    "with open(f'{exp_dir}/config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "start_dim = config['start_dim']\n",
    "\n",
    "feature_dim = config['feature_dim']\n",
    "evecs_per_support = config['evecs_per_support']\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = diffusion_network.DiffusionNet(\n",
    "    **config['net_params']\n",
    "    ).to(device)\n",
    "\n",
    "input_type = config['net_params']['input_type']\n",
    "\n",
    "\n",
    "log_file = f'{exp_dir}/log_landmarks.txt'\n",
    "\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "\n",
    "\n",
    "dataset_list = [\n",
    "    # (config[\"train_folder\"], 'train'),\n",
    "    \n",
    "    ('FAUST_a', 'test'),\n",
    "    ('SHREC19_r', 'test'), \n",
    "    ('FAUST_r', 'test'),\n",
    "    ('SCAPE_r_pair', 'test'),\n",
    "    ('SCAPE_a_pair', 'test'),\n",
    "]\n",
    "    \n",
    "# find the latest checkpoint in f'{exp_dir}/....pth'\n",
    "checkpoint_files = os.listdir(exp_dir)\n",
    "checkpoint_files = [f for f in checkpoint_files if f.endswith('.pth')]\n",
    "checkpoint_files = [int(f.split('.')[0]) for f in checkpoint_files]\n",
    "checkpoint_files = sorted(checkpoint_files)\n",
    "\n",
    "last_checkpoint = checkpoint_files[-1]\n",
    "\n",
    "\n",
    "for n_iter in [last_checkpoint]:\n",
    "# for n_iter in [200, 600, 1000, 1400, 2000]:\n",
    "\n",
    "    net.load_state_dict(torch.load(f'{exp_dir}/{n_iter}.pth'))\n",
    "\n",
    "\n",
    "    for dataset_name, split in dataset_list:\n",
    "        \n",
    "        test_dataset_curr = data_loading.get_val_dataset(\n",
    "            dataset_name, split, 128, canonicalize_fmap=None, preload=False, return_evecs=True, centering='mean'\n",
    "            )[0]\n",
    "            \n",
    "        mean_incorrect_signs, max_incorrect_signs = test_on_dataset(\n",
    "            test_dataset_curr, n_epochs=100, config=config)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sign_change_landmarks(evecs_flip, landmarks_vector):\n",
    "    \n",
    "    assert evecs_flip.dim() == 3\n",
    "    assert landmarks_vector.dim() == 3\n",
    "    \n",
    "    # normalize the evecs\n",
    "    evecs_flip = torch.nn.functional.normalize(evecs_flip, p=2, dim=1)\n",
    "    \n",
    "    # normalize the support vector\n",
    "    landmarks_vector_norm = torch.nn.functional.normalize(landmarks_vector, p=2, dim=1)\n",
    "        \n",
    "    # multiply the support vector by the flipped evecs \n",
    "    # [1 x 6890 x 4].T @ [1 x 6890 x 6890] @ [1 x 6890 x 4]\n",
    "    \n",
    "    product_with_support = landmarks_vector_norm.transpose(1, 2) @ evecs_flip\n",
    "    \n",
    "    \n",
    "    assert product_with_support.shape[1] == product_with_support.shape[2]\n",
    "    \n",
    "    # take the sign of diagonal elements\n",
    "    sign_flip_predicted = torch.diagonal(product_with_support, dim1=1, dim2=2)\n",
    " \n",
    "    return sign_flip_predicted, None, product_with_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_on_dataset(test_dataset, n_epochs, feature_dim, n_landmarks):\n",
    "\n",
    "    tqdm._instances.clear()\n",
    "        \n",
    "    iterator = tqdm(total=len(test_dataset) * n_epochs)\n",
    "    incorrect_signs_list = torch.tensor([])\n",
    "    curr_iter = 0\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    corr_shape = test_dataset[0]['corr'].shape\n",
    "    \n",
    "    # choose the landmarks\n",
    "    landmarks_on_template = torch.tensor(np.random.choice(corr_shape[0], n_landmarks, replace=False))  \n",
    "    \n",
    "\n",
    "    for _ in range(n_epochs):\n",
    "        for curr_idx in range(len(test_dataset)):\n",
    "\n",
    "            ##############################################\n",
    "            # Select a shape\n",
    "            ##############################################\n",
    "\n",
    "            train_shape = test_dataset[curr_idx]  \n",
    "            \n",
    "            assert train_shape['corr'].shape == corr_shape    \n",
    "            \n",
    "            ##############################################\n",
    "            # Set the variables\n",
    "            ##############################################\n",
    "\n",
    "            evecs_orig = train_shape['evecs'].unsqueeze(0)[:, :, :feature_dim].to(device)\n",
    "            \n",
    "            ##############################################\n",
    "            # Get the landmarks on the shape\n",
    "            ##############################################\n",
    "            \n",
    "            landmarks_on_shape = train_shape['corr'][landmarks_on_template]\n",
    "            \n",
    "            # vector where the landmarks are 1 and the rest is 0, shape [1 x 6890 x 32]\n",
    "            landmarks_vector = torch.zeros_like(evecs_orig)\n",
    "            landmarks_vector[:, landmarks_on_shape] = 1   \n",
    "            \n",
    "            # print(landmarks_on_shape)\n",
    "            \n",
    "            # print(landmarks_vector.shape, landmarks_vector)       \n",
    "            \n",
    "            # return\n",
    "            \n",
    "            ##############################################\n",
    "            # Set the signs on shape 0\n",
    "            ##############################################\n",
    "\n",
    "            # create a random combilation of +1 and -1, length = feature_dim\n",
    "            sign_gt_0 = torch.randint(0, 2, (feature_dim,)).float().to(device)\n",
    "            \n",
    "            sign_gt_0[sign_gt_0 == 0] = -1\n",
    "            sign_gt_0 = sign_gt_0.float().unsqueeze(0)\n",
    "\n",
    "            # print('evecs_orig', evecs_orig.shape, 'sign_gt_0', sign_gt_0.shape)\n",
    "\n",
    "            # multiply evecs [6890 x 16] by sign_flip [16]\n",
    "            evecs_flip_0 = evecs_orig * sign_gt_0\n",
    "            \n",
    "            \n",
    "            \n",
    "            # predict the sign change\n",
    "            sign_pred_0, _, _ = predict_sign_change_landmarks(\n",
    "                evecs_flip_0, landmarks_vector\n",
    "                )\n",
    "            \n",
    "            ##############################################\n",
    "            # Set the signs on shape 1\n",
    "            ##############################################\n",
    "            \n",
    "            # create a random combilation of +1 and -1, length = feature_dim\n",
    "            sign_gt_1 = torch.randint(0, 2, (feature_dim,)).float().to(device)\n",
    "            \n",
    "            sign_gt_1[sign_gt_1 == 0] = -1\n",
    "            sign_gt_1 = sign_gt_1.float().unsqueeze(0)\n",
    "            \n",
    "            # multiply evecs [6890 x 16] by sign_flip [16]\n",
    "            evecs_flip_1 = evecs_orig * sign_gt_1\n",
    "            \n",
    "            # predict the sign change\n",
    "            sign_pred_1, _, _ = predict_sign_change_landmarks(\n",
    "                evecs_flip_1, landmarks_vector\n",
    "                )\n",
    "            \n",
    "            ##############################################\n",
    "            # Calculate the loss\n",
    "            ##############################################\n",
    "            \n",
    "            # calculate the ground truth sign difference\n",
    "            sign_diff_gt = sign_gt_1 * sign_gt_0\n",
    "            \n",
    "            # calculate the sign difference between predicted evecs\n",
    "            sign_diff_pred = sign_pred_1 * sign_pred_0\n",
    "            \n",
    "            sign_correct = sign_diff_pred.sign() * sign_diff_gt.sign() \n",
    "            \n",
    "            \n",
    "            # count the number of incorrect signs\n",
    "            count_incorrect_signs = (sign_correct < 0).int().sum()\n",
    "                \n",
    "            # incorrect_signs_list.append(count_incorrect_signs)\n",
    "            incorrect_signs_list = torch.cat([incorrect_signs_list, torch.tensor([count_incorrect_signs])])\n",
    "            \n",
    "            \n",
    "            # print(f'count_incorrect_signs {count_incorrect_signs}')\n",
    "            # return\n",
    "            \n",
    "            iterator.set_description(f'Mean incorrect signs {incorrect_signs_list.float().mean():.2f} / {feature_dim}, max {incorrect_signs_list.max()}')\n",
    "            iterator.update(1)\n",
    "            # if count_incorrect_signs > 7:\n",
    "            #     raise ValueError('Too many incorrect signs')\n",
    "        \n",
    "    return incorrect_signs_list.float().mean(), incorrect_signs_list.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_curr = data_loading.get_val_dataset(\n",
    "    'FAUST_r', 'test', 128, canonicalize_fmap=None, preload=False, return_evecs=True, centering='mean'\n",
    "    )[0]\n",
    "    \n",
    "test_dataset_curr = [test_dataset_curr[i] for i in range(len(test_dataset_curr))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_incorrect_signs, max_incorrect_signs = test_on_dataset(\n",
    "    test_dataset_curr, n_epochs=100,\n",
    "    feature_dim=96,\n",
    "    n_landmarks=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
