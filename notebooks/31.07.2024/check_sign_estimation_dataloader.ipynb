{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import trimesh\n",
    "\n",
    "scene = trimesh.Scene()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_code.datasets.surreal_dataset_3dc import TemplateSurrealDataset3DC\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "dataset_3dc = TemplateSurrealDataset3DC(\n",
    "    shape_path=f'/home/s94zalek_hpc/3D-CODED/data/mmap_datas_surreal_train.pth',\n",
    "    # shape_path='/lustre/mlnvme/data/s94zalek_hpc-shape_matching/mmap_datas_surreal_train.pth',\n",
    "    num_evecs=128,\n",
    "    use_cuda=False,\n",
    "    cache_lb_dir=None,\n",
    "    return_evecs=True,\n",
    "    mmap=True\n",
    ")    \n",
    "\n",
    "dataloader_3dc = torch.utils.data.DataLoader(\n",
    "    dataset_3dc, batch_size=1, shuffle=False,\n",
    "    num_workers=0,\n",
    "    # persistent_workers=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networks.diffusion_network as diffusion_network\n",
    "\n",
    "condition_dim = 0\n",
    "start_dim = 0\n",
    "\n",
    "feature_dim = 32\n",
    "evecs_per_support = 4\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = diffusion_network.DiffusionNet(\n",
    "    in_channels=feature_dim,\n",
    "    out_channels=feature_dim // evecs_per_support,\n",
    "    cache_dir=None,\n",
    "    input_type='wks',\n",
    "    k_eig=128,\n",
    "    n_block=6\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_type = 'wks'\n",
    "net.load_state_dict(torch.load('/home/s94zalek_hpc/shape_matching/my_code/experiments/sign_double_start_0_feat_32_6block_factor4_dataset_SURREAL_train_rot_180_180_180_normal_True_noise_0.0_-0.05_0.05_lapl_mesh_scale_0.9_1.1_wks/40000.pth'))\n",
    "# net.load_state_dict(torch.load('/home/s94zalek_hpc/shape_matching/my_code/experiments/sign_estimator_no_aug/40000.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_3dc))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id torch.Size([1])\n",
      "verts torch.Size([1, 6890, 3])\n",
      "faces torch.Size([1, 13776, 3])\n",
      "corr torch.Size([1, 6890])\n",
      "evecs torch.Size([1, 6890, 32])\n",
      "evecs_trans torch.Size([1, 32, 6890])\n",
      "evals torch.Size([1, 1, 32])\n",
      "mass torch.Size([1, 6890])\n",
      "L torch.Size([1, 6890, 6890])\n",
      "gradX torch.Size([1, 6890, 6890])\n",
      "gradY torch.Size([1, 6890, 6890])\n"
     ]
    }
   ],
   "source": [
    "for k in batch['first'].keys():\n",
    "    print(k, batch['first'][k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 4 random training shapes to trimesh scene\n",
    "\n",
    "# np.random.shuffle(train_shapes)\n",
    "scene.geometry.clear()\n",
    "\n",
    "max_shapes = 5\n",
    "for i, batch in enumerate(tqdm(dataloader_3dc)):\n",
    "    scene.add_geometry(trimesh.Trimesh(\n",
    "        vertices=batch['second']['verts'][0] + torch.tensor([i, 0, 0]),\n",
    "        faces=batch['second']['faces'][0]))\n",
    "    max_shapes -= 1\n",
    "    if max_shapes == 0:\n",
    "        break\n",
    "    \n",
    "axis = trimesh.creation.axis(axis_length=1)\n",
    "scene.add_geometry(axis)\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean incorrect signs 0.23 / 32, max 3.0:   0%|                                                      | 35/230000 [01:38<180:09:16,  2.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m incorrect_signs_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([])\n\u001b[1;32m     13\u001b[0m curr_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:     \n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m##############################################\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Select a shape\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m##############################################\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     verts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverts\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m     faces \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfaces\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/fmnet/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/fmnet/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/fmnet/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/fmnet/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/fmnet/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/shape_matching/my_code/datasets/surreal_dataset_3dc.py:116\u001b[0m, in \u001b[0;36mTemplateSurrealDataset3DC.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# get eigenfunctions/eigenvalues\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_evecs:\n\u001b[0;32m--> 116\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spectral_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_evecs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_evecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_lb_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# 1 to 1 correspondence\u001b[39;00m\n\u001b[1;32m    119\u001b[0m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverts\u001b[39m\u001b[38;5;124m'\u001b[39m]))))        \n",
      "File \u001b[0;32m~/shape_matching/my_code/datasets/preprocessing.py:89\u001b[0m, in \u001b[0;36mget_spectral_ops\u001b[0;34m(item, num_evecs, cache_dir)\u001b[0m\n\u001b[1;32m     77\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(cache_dir)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# _, mass, L, evals, evecs, _, _ = get_operators(item['verts'], item.get('faces'),\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#                                                k=num_evecs,\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#                                                cache_dir=cache_dir)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# item['mass'] = mass\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# item['L'] = L.to_dense()\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m _, mass, L, evals, evecs, gradX, gradY \u001b[38;5;241m=\u001b[39m \u001b[43mget_operators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mverts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfaces\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_evecs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# evals = evals.unsqueeze(0)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m evecs_trans \u001b[38;5;241m=\u001b[39m evecs\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m*\u001b[39m mass[\u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/shape_matching/utils/geometry_util.py:732\u001b[0m, in \u001b[0;36mget_operators\u001b[0;34m(verts, faces, k, normals, cache_dir, overwrite_cache)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m found:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# recompute\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m     frames, mass, L, evals, evecs, gradX, gradY \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_operators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     dtype_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;66;03m# save\u001b[39;00m\n",
      "File \u001b[0;32m~/shape_matching/utils/geometry_util.py:646\u001b[0m, in \u001b[0;36mcompute_operators\u001b[0;34m(verts, faces, k, normals)\u001b[0m\n\u001b[1;32m    644\u001b[0m evecs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(evecs_np)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    645\u001b[0m gradX \u001b[38;5;241m=\u001b[39m sparse_np_to_torch(gradX_np)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 646\u001b[0m gradY \u001b[38;5;241m=\u001b[39m \u001b[43msparse_np_to_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradY_np\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frames, massvec, L, evals, evecs, gradX, gradY\n",
      "File \u001b[0;32m~/shape_matching/utils/geometry_util.py:43\u001b[0m, in \u001b[0;36msparse_np_to_torch\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     41\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((Acoo\u001b[38;5;241m.\u001b[39mrow, Acoo\u001b[38;5;241m.\u001b[39mcol))\n\u001b[1;32m     42\u001b[0m shape \u001b[38;5;241m=\u001b[39m Acoo\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_coo_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import utils.geometry_util as geometry_util\n",
    "import robust_laplacian\n",
    "import scipy.sparse.linalg as sla\n",
    "import utils.geometry_util as geometry_util\n",
    "import my_code.sign_canonicalization.training as sign_training\n",
    "\n",
    "\n",
    "tqdm._instances.clear()\n",
    "       \n",
    "iterator = tqdm(dataloader_3dc, total=len(dataset_3dc))\n",
    "incorrect_signs_list = torch.tensor([])\n",
    "curr_iter = 0\n",
    "\n",
    "    \n",
    "for batch in iterator:     \n",
    "\n",
    "    ##############################################\n",
    "    # Select a shape\n",
    "    ##############################################\n",
    "    \n",
    "    verts = batch['second']['verts'].to(device)\n",
    "    faces = batch['second']['faces'].to(device)\n",
    "    evecs_orig = batch['second']['evecs'][:, :, start_dim:start_dim+feature_dim].to(device)\n",
    "\n",
    "    ##############################################\n",
    "    # Set the signs on shape 0\n",
    "    ##############################################\n",
    "\n",
    "    # create a random combilation of +1 and -1, length = feature_dim\n",
    "    sign_gt_0 = torch.randint(0, 2, (feature_dim,)).float().to(device)\n",
    "    \n",
    "    sign_gt_0[sign_gt_0 == 0] = -1\n",
    "    sign_gt_0 = sign_gt_0.float().unsqueeze(0)\n",
    "\n",
    "    # multiply evecs [6890 x 16] by sign_flip [16]\n",
    "    evecs_flip_0 = evecs_orig * sign_gt_0\n",
    "    \n",
    "    # predict the sign change\n",
    "    with torch.no_grad():\n",
    "        sign_pred_0, supp_vec_0, _ = sign_training.predict_sign_change(\n",
    "            net, verts, faces, evecs_flip_0, evecs_cond=None, input_type=input_type,\n",
    "            mass=batch['second']['mass'], L=batch['second']['L'],\n",
    "            evals=batch['second']['evals'], evecs=batch['second']['evecs'],\n",
    "            gradX=batch['second']['gradX'], gradY=batch['second']['gradY']\n",
    "            )\n",
    "    \n",
    "    ##############################################\n",
    "    # Set the signs on shape 1\n",
    "    ##############################################\n",
    "    \n",
    "    # create a random combilation of +1 and -1, length = feature_dim\n",
    "    sign_gt_1 = torch.randint(0, 2, (feature_dim,)).float().to(device)\n",
    "    \n",
    "    sign_gt_1[sign_gt_1 == 0] = -1\n",
    "    sign_gt_1 = sign_gt_1.float().unsqueeze(0)\n",
    "    \n",
    "    # multiply evecs [6890 x 16] by sign_flip [16]\n",
    "    evecs_flip_1 = evecs_orig * sign_gt_1\n",
    "    \n",
    "    # predict the sign change\n",
    "    with torch.no_grad():\n",
    "        sign_pred_1, supp_vec_1, _ = sign_training.predict_sign_change(\n",
    "            net, verts, faces, evecs_flip_1, evecs_cond=None, input_type=input_type,\n",
    "            mass=batch['second']['mass'], L=batch['second']['L'],\n",
    "            evals=batch['second']['evals'], evecs=batch['second']['evecs'],\n",
    "            gradX=batch['second']['gradX'], gradY=batch['second']['gradY']\n",
    "            )\n",
    "    \n",
    "    ##############################################\n",
    "    # Calculate the loss\n",
    "    ##############################################\n",
    "    \n",
    "    # calculate the ground truth sign difference\n",
    "    sign_diff_gt = sign_gt_1 * sign_gt_0\n",
    "    \n",
    "    # calculate the sign difference between predicted evecs\n",
    "    sign_diff_pred = sign_pred_1 * sign_pred_0\n",
    "    \n",
    "    sign_correct = sign_diff_pred.sign() * sign_diff_gt.sign() \n",
    "    \n",
    "    \n",
    "    # count the number of incorrect signs\n",
    "    count_incorrect_signs = (sign_correct < 0).int().sum()\n",
    "        \n",
    "    # incorrect_signs_list.append(count_incorrect_signs)\n",
    "    incorrect_signs_list = torch.cat([incorrect_signs_list, torch.tensor([count_incorrect_signs])])\n",
    "    \n",
    "    \n",
    "    iterator.set_description(f'Mean incorrect signs {incorrect_signs_list.float().mean():.2f} / {feature_dim}, max {incorrect_signs_list.max()}')\n",
    "    iterator.update(1)\n",
    "    # if count_incorrect_signs > 7:\n",
    "    #     raise ValueError('Too many incorrect signs')\n",
    "    \n",
    "    \n",
    "print(f'Results for {len(incorrect_signs_list)} test shapes')\n",
    "print(f'Incorrect signs per shape: {incorrect_signs_list.float().mean():.2f} / {feature_dim}')\n",
    "\n",
    "print('Max incorrect signs', incorrect_signs_list.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).unsqueeze(0)\n",
    "\n",
    "torch.diag_embed(t).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
