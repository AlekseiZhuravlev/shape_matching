{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# models\n",
    "from my_code.models.diag_conditional import DiagConditionedUnet\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "import my_code.datasets.template_dataset as template_dataset\n",
    "\n",
    "import my_code.diffusion_training_sign_corr.data_loading as data_loading\n",
    "\n",
    "import networks.diffusion_network as diffusion_network\n",
    "import matplotlib.pyplot as plt\n",
    "import my_code.utils.plotting_utils as plotting_utils\n",
    "import utils.fmap_util as fmap_util\n",
    "import metrics.geodist_metric as geodist_metric\n",
    "from my_code.sign_canonicalization.training import predict_sign_change\n",
    "import argparse\n",
    "from pyFM_fork.pyFM.refine.zoomout import zoomout_refine\n",
    "import my_code.utils.zoomout_custom as zoomout_custom\n",
    "\n",
    "tqdm._instances.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# configuration\n",
    "experiment_name = 'augShapes_mass_signNet_remeshed_10_0.2_0.8'\n",
    "checkpoint_name = 'checkpoint_99.pt'\n",
    "\n",
    "### config\n",
    "exp_base_folder = f'/home/s94zalek_hpc/shape_matching/my_code/experiments/ddpm/{experiment_name}'\n",
    "with open(f'{exp_base_folder}/config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "### model\n",
    "model = DiagConditionedUnet(config[\"model_params\"]).to('cuda')\n",
    "model.load_state_dict(torch.load(f\"{exp_base_folder}/checkpoints/{checkpoint_name}\"))\n",
    "model = model.to('cuda')\n",
    "\n",
    "### Sign correction network\n",
    "sign_corr_net = diffusion_network.DiffusionNet(\n",
    "    **config[\"sign_net\"][\"net_params\"]\n",
    "    ).to('cuda')\n",
    "    \n",
    "sign_corr_net.load_state_dict(torch.load(\n",
    "        f'{config[\"sign_net\"][\"net_path\"]}/{config[\"sign_net\"][\"n_iter\"]}.pth'\n",
    "        ))\n",
    "\n",
    "\n",
    "### sample the model\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2',\n",
    "                                clip_sample=True) \n",
    "\n",
    "\n",
    "### test dataset\n",
    "dataset_name = 'FAUST_r_pair'\n",
    "split = 'test'\n",
    "\n",
    "single_dataset, test_dataset = data_loading.get_val_dataset(\n",
    "    dataset_name, split, 200, preload=False, return_evecs=True\n",
    "    )\n",
    "sign_corr_net.cache_dir = single_dataset.lb_cache_dir\n",
    "\n",
    "\n",
    "num_evecs = config[\"model_params\"][\"sample_size\"]\n",
    "\n",
    "##########################################\n",
    "# Template\n",
    "##########################################\n",
    "\n",
    "template_shape = template_dataset.get_template(\n",
    "    template_path='data/SURREAL_full/template/template.ply',\n",
    "    num_evecs=single_dataset.num_evecs,\n",
    "    template_corr=list(range(6890)),\n",
    "    centering='bbox',\n",
    "    )    \n",
    "\n",
    "##########################################\n",
    "# Logging\n",
    "##########################################\n",
    "\n",
    "log_dir = f'{exp_base_folder}/eval/{checkpoint_name}/{dataset_name}-{split}-template'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "fig_dir = f'{log_dir}/figs'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "log_file_name = f'{log_dir}/log.txt'\n",
    "\n",
    "##########################################\n",
    "\n",
    "ratios = []\n",
    "geo_errs = []\n",
    "geo_errs_zo = []\n",
    "\n",
    "Cxy_est_list = []\n",
    "C_gt_xy_corr_list = []\n",
    "\n",
    "\n",
    "data_range = tqdm(range(len(single_dataset)), desc='Calculating fmaps to template')\n",
    "\n",
    "for i in data_range:\n",
    "\n",
    "    data = single_dataset[i]\n",
    "    \n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    \n",
    "    verts_first = template_shape['verts'].unsqueeze(0).to(device)\n",
    "    verts_second = data['verts'].unsqueeze(0).to(device)\n",
    "    \n",
    "    faces_first = template_shape['faces'].unsqueeze(0).to(device)\n",
    "    faces_second = data['faces'].unsqueeze(0).to(device)\n",
    "\n",
    "    evecs_first = template_shape['evecs'][:, :num_evecs].unsqueeze(0).to(device)\n",
    "    evecs_second = data['evecs'][:, :num_evecs].unsqueeze(0).to(device)\n",
    "    \n",
    "    evals_first = template_shape['evals'][:num_evecs]\n",
    "    evals_second = data['evals'][:num_evecs]\n",
    "\n",
    "    # corr_first = data['first']['corr']\n",
    "    # corr_second = data['corr']\n",
    "    \n",
    "    if config[\"sign_net\"][\"with_mass\"]:\n",
    "        mass_mat_first = torch.diag_embed(\n",
    "            template_shape['mass'].unsqueeze(0)\n",
    "            ).to(device)\n",
    "        mass_mat_second = torch.diag_embed(\n",
    "            data['mass'].unsqueeze(0)\n",
    "            ).to(device)\n",
    "    else:\n",
    "        mass_mat_first = None\n",
    "        mass_mat_second = None\n",
    "\n",
    "\n",
    "    # predict the sign change\n",
    "    with torch.no_grad():\n",
    "        sign_pred_first, support_vector_norm_first, _ = predict_sign_change(\n",
    "            sign_corr_net, verts_first, faces_first, evecs_first, \n",
    "            mass_mat=mass_mat_first, input_type=sign_corr_net.input_type,\n",
    "            # mass=None, L=None, evals=None, evecs=None, gradX=None, gradY=None\n",
    "            mass=template_shape['mass'].unsqueeze(0), L=template_shape['L'].unsqueeze(0),\n",
    "            evals=template_shape['evals'][:config[\"sign_net\"][\"net_params\"][\"k_eig\"]].unsqueeze(0),\n",
    "            evecs=template_shape['evecs'][:,:config[\"sign_net\"][\"net_params\"][\"k_eig\"]].unsqueeze(0),\n",
    "            gradX=template_shape['gradX'].unsqueeze(0), gradY=template_shape['gradY'].unsqueeze(0)\n",
    "            )\n",
    "        sign_pred_second, support_vector_norm_second, _ = predict_sign_change(\n",
    "            sign_corr_net, verts_second, faces_second, evecs_second, \n",
    "            mass_mat=mass_mat_second, input_type=sign_corr_net.input_type,\n",
    "            # mass=None, L=None, evals=None, evecs=None, gradX=None, gradY=None\n",
    "            mass=data['mass'].unsqueeze(0), L=data['L'].unsqueeze(0),\n",
    "            evals=data['evals'][:config[\"sign_net\"][\"net_params\"][\"k_eig\"]].unsqueeze(0),\n",
    "            evecs=data['evecs'][:,:config[\"sign_net\"][\"net_params\"][\"k_eig\"]].unsqueeze(0),\n",
    "            gradX=data['gradX'].unsqueeze(0), gradY=data['gradY'].unsqueeze(0)\n",
    "            )\n",
    "\n",
    "    # correct the evecs\n",
    "    evecs_first_corrected = evecs_first.cpu()[0] * torch.sign(sign_pred_first).cpu()\n",
    "    evecs_first_corrected_norm = evecs_first_corrected / torch.norm(evecs_first_corrected, dim=0, keepdim=True)\n",
    "    \n",
    "    evecs_second_corrected = evecs_second.cpu()[0] * torch.sign(sign_pred_second).cpu()\n",
    "    evecs_second_corrected_norm = evecs_second_corrected / torch.norm(evecs_second_corrected, dim=0, keepdim=True)\n",
    "    \n",
    "    # product with support\n",
    "    # evecs_cond_first = evecs_first_corrected_norm.transpose(0, 1) @ support_vector_norm_first[0].cpu()\n",
    "    # evecs_cond_second = evecs_second_corrected_norm.transpose(0, 1) @ support_vector_norm_second[0].cpu()\n",
    "\n",
    "\n",
    "    # product with support\n",
    "    # if config[\"sign_net\"][\"with_mass\"]:\n",
    "    if config[\"sign_net\"]['cond_mass_normalize']:\n",
    "        \n",
    "        mass_mat_first = torch.diag_embed(\n",
    "            template_shape['mass'].unsqueeze(0)\n",
    "            ).to(device)\n",
    "        mass_mat_second = torch.diag_embed(\n",
    "            data['mass'].unsqueeze(0)\n",
    "            ).to(device)\n",
    "        \n",
    "        evecs_cond_first = torch.nn.functional.normalize(\n",
    "            support_vector_norm_first[0].cpu().transpose(0, 1) \\\n",
    "                @ mass_mat_first[0].cpu(),\n",
    "            p=2, dim=1) \\\n",
    "                @ evecs_first_corrected_norm\n",
    "        \n",
    "        evecs_cond_second = torch.nn.functional.normalize(\n",
    "            support_vector_norm_second[0].cpu().transpose(0, 1) \\\n",
    "                @ mass_mat_second[0].cpu(),\n",
    "            p=2, dim=1) \\\n",
    "                @ evecs_second_corrected_norm \n",
    "        \n",
    "    else:\n",
    "        evecs_cond_first = support_vector_norm_first[0].cpu().transpose(0, 1) @ evecs_first_corrected_norm\n",
    "        evecs_cond_second = support_vector_norm_second[0].cpu().transpose(0, 1) @ evecs_second_corrected_norm\n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    # Conditioning\n",
    "    ###############################################\n",
    "\n",
    "    conditioning = torch.tensor([])\n",
    "    \n",
    "    if 'evals' in config[\"conditioning_types\"]:\n",
    "        eval = evals_second.unsqueeze(0)\n",
    "        eval = torch.diag_embed(eval)\n",
    "        conditioning = torch.cat((conditioning, eval), 0)\n",
    "    \n",
    "    if 'evals_inv' in config[\"conditioning_types\"]:\n",
    "        eval_inv = 1 / evals_second.unsqueeze(0)\n",
    "        # replace elements > 1 with 1\n",
    "        eval_inv[eval_inv > 1] = 1\n",
    "        eval_inv = torch.diag_embed(eval_inv)\n",
    "        conditioning = torch.cat((conditioning, eval_inv), 0)\n",
    "    \n",
    "    if 'evecs' in config[\"conditioning_types\"]:\n",
    "        evecs = torch.cat(\n",
    "            (evecs_cond_first.unsqueeze(0), evecs_cond_second.unsqueeze(0)),\n",
    "            0)\n",
    "        conditioning = torch.cat((conditioning, evecs), 0)\n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    # Sample the model\n",
    "    ###############################################\n",
    "    \n",
    "    x_sampled = torch.rand(1, 1, model.model.sample_size, model.model.sample_size).to(device)\n",
    "    y = conditioning.unsqueeze(0).to(device)    \n",
    "    \n",
    "    # print(x_sampled.shape, y.shape)\n",
    "        \n",
    "    # Sampling loop\n",
    "    for t in noise_scheduler.timesteps:\n",
    "\n",
    "        # Get model pred\n",
    "        with torch.no_grad():\n",
    "            residual = model(x_sampled, t,\n",
    "                                conditioning=y\n",
    "                                ).sample\n",
    "\n",
    "        # Update sample with step\n",
    "        x_sampled = noise_scheduler.step(residual, t, x_sampled).prev_sample\n",
    "\n",
    "    Cxy_est = x_sampled[0][0].cpu()\n",
    "    \n",
    "    ###############################################\n",
    "    # Zoomout\n",
    "    ###############################################\n",
    "    \n",
    "    evecs_first_zo = torch.cat(\n",
    "        [evecs_first_corrected,\n",
    "            template_shape['evecs'][:, num_evecs:]], 1)\n",
    "    evecs_second_zo = torch.cat(\n",
    "        [evecs_second_corrected,\n",
    "            data['evecs'][:, num_evecs:]], 1)\n",
    "    \n",
    "    # assert (evecs_first_zo.shape[1] - num_evecs) % 8 == 0, f'Number of evecs {evecs_first_zo.shape[1] - num_evecs} must be divisible by 8'\n",
    "    \n",
    "    # C_xy_est_zo = torch.tensor(zoomout_refine(\n",
    "    #         FM_12=Cxy_est.numpy(), \n",
    "    #         evects1=evecs_first_zo.numpy(), \n",
    "    #         evects2=evecs_second_zo.numpy(),\n",
    "    #         nit=8, step=(evecs_first_zo.shape[1] - num_evecs) // 8,\n",
    "    #         verbose=False\n",
    "    #     ))\n",
    "    \n",
    "    C_xy_est_zo = zoomout_custom.zoomout(\n",
    "        FM_12=Cxy_est.to(device), \n",
    "        evects1=evecs_first_zo.to(device), \n",
    "        evects2=evecs_second_zo.to(device),\n",
    "        nit=evecs_first_zo.shape[1] - num_evecs, step=1,\n",
    "        # nit=8, step=(evecs_first_zo.shape[1] - num_evecs) // 8,\n",
    "    ).cpu()\n",
    "    \n",
    "    p2p_est = fmap_util.fmap2pointmap(\n",
    "        C12=Cxy_est.to(device),\n",
    "        evecs_x=evecs_first.to(device)[0],\n",
    "        evecs_y=evecs_second.to(device)[0],\n",
    "        ).cpu()\n",
    "    \n",
    "    p2p_est_zo = fmap_util.fmap2pointmap(\n",
    "        C12=C_xy_est_zo.to(device),\n",
    "        evecs_x=evecs_first_zo.to(device),\n",
    "        evecs_y=evecs_second_zo.to(device),\n",
    "        ).cpu()\n",
    "    \n",
    "    single_dataset.additional_data[i]['C_xy_est'] = Cxy_est\n",
    "    single_dataset.additional_data[i]['C_xy_est_zo'] = C_xy_est_zo\n",
    "    single_dataset.additional_data[i]['evecs_zo'] = evecs_second_zo\n",
    "    \n",
    "    single_dataset.additional_data[i]['p2p_est'] = p2p_est\n",
    "    single_dataset.additional_data[i]['p2p_est_zo'] = p2p_est_zo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dataset[3]['p2p_est'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dataset[6]['corr'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(5, 3))\n",
    "\n",
    "l = 0\n",
    "h = 32\n",
    "\n",
    "plotting_utils.plot_Cxy(fig, axs[0], single_dataset[9]['C_xy_est'],\n",
    "                        f'Cxy_est', l, h, show_grid=False, show_colorbar=False)\n",
    "plotting_utils.plot_Cxy(fig, axs[1], single_dataset[9]['C_xy_est_zo'],\n",
    "                        f'Cxy_est_zo', l, h, show_grid=False, show_colorbar=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.dataset = single_dataset\n",
    "    \n",
    "data_range_pair = tqdm(range(len(test_dataset)), desc='Calculating pair fmaps')\n",
    "\n",
    "for i in data_range_pair:\n",
    "    \n",
    "    data = test_dataset[i]\n",
    "    \n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # device = 'cpu'\n",
    "    \n",
    "    verts_first = data['first']['verts'].unsqueeze(0).to(device)\n",
    "    verts_second = data['second']['verts'].unsqueeze(0).to(device)\n",
    "    \n",
    "    faces_first = data['first']['faces'].unsqueeze(0).to(device)\n",
    "    faces_second = data['second']['faces'].unsqueeze(0).to(device)\n",
    "\n",
    "    # evecs_first = data['first']['evecs'][:, :num_evecs].unsqueeze(0).to(device)\n",
    "    # evecs_second = data['second']['evecs'][:, :num_evecs].unsqueeze(0).to(device)\n",
    "    \n",
    "    evecs_first = data['first']['evecs'][:, :].unsqueeze(0).to(device)\n",
    "    evecs_second = data['second']['evecs'][:, :].unsqueeze(0).to(device)\n",
    "    \n",
    "    evals_first = data['first']['evals'][:num_evecs]\n",
    "    evals_second = data['second']['evals'][:num_evecs]\n",
    "\n",
    "    corr_first = data['first']['corr']\n",
    "    corr_second = data['second']['corr']\n",
    "    \n",
    "    ###############################################\n",
    "    # Functional maps\n",
    "    ###############################################\n",
    "    \n",
    "    evecs_first_zo = data['first']['evecs_zo']\n",
    "    evecs_second_zo = data['second']['evecs_zo']\n",
    "    \n",
    "    C_gt_xy = torch.linalg.lstsq(\n",
    "        evecs_second.cpu()[0, corr_second],\n",
    "        evecs_first.cpu()[0, corr_first]\n",
    "        ).solution\n",
    "    \n",
    "    C_gt_xy_corr = torch.linalg.lstsq(\n",
    "            evecs_second_zo[corr_second],\n",
    "            evecs_first_zo[corr_first]\n",
    "            ).solution\n",
    "    \n",
    "    Cxy_est_first = data['first']['C_xy_est']\n",
    "    Cxy_est_second = data['second']['C_xy_est']\n",
    "    \n",
    "    C_xy_est_zo_first = data['first']['C_xy_est_zo']\n",
    "    C_xy_est_zo_second = data['second']['C_xy_est_zo']\n",
    "    \n",
    "    # C_xy_est = torch.linalg.lstsq(Cxy_est_first, Cxy_est_second).solution\n",
    "    # C_xy_est_zo = torch.linalg.lstsq(C_xy_est_zo_first, C_xy_est_zo_second).solution\n",
    "    \n",
    "    C_xy_est = torch.linalg.pinv(Cxy_est_first) @ Cxy_est_second\n",
    "    C_xy_est_zo = torch.linalg.pinv(C_xy_est_zo_first) @ C_xy_est_zo_second\n",
    "        \n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    # Evaluation\n",
    "    ###############################################  \n",
    "    \n",
    "    # hard correspondence \n",
    "    p2p_gt = fmap_util.fmap2pointmap(\n",
    "        C12=C_gt_xy,\n",
    "        evecs_x=evecs_first.cpu()[0],\n",
    "        evecs_y=evecs_second.cpu()[0],\n",
    "        )\n",
    "    p2p_corr_gt = fmap_util.fmap2pointmap(\n",
    "        C12=C_gt_xy_corr,\n",
    "        evecs_x=evecs_first_zo,\n",
    "        evecs_y=evecs_second_zo,\n",
    "        )\n",
    "    p2p_est = fmap_util.fmap2pointmap(\n",
    "        C_xy_est,\n",
    "        evecs_x=evecs_first_zo[:, :num_evecs],\n",
    "        evecs_y=evecs_second_zo[:, :num_evecs],\n",
    "        )\n",
    "    p2p_est_zo = fmap_util.fmap2pointmap(\n",
    "        C_xy_est_zo,\n",
    "        evecs_x=evecs_first_zo,\n",
    "        evecs_y=evecs_second_zo,\n",
    "        )\n",
    "    \n",
    "    # distance matrices\n",
    "    dist_x = torch.cdist(data['first']['verts'], data['first']['verts'])\n",
    "    dist_y = torch.cdist(data['second']['verts'], data['second']['verts'])\n",
    "\n",
    "    # geodesic error\n",
    "    geo_err_gt = geodist_metric.calculate_geodesic_error(\n",
    "        dist_x, data['first']['corr'], data['second']['corr'], p2p_gt, return_mean=False\n",
    "        )  \n",
    "    geo_err_corr_gt = geodist_metric.calculate_geodesic_error(\n",
    "        dist_x, data['first']['corr'], data['second']['corr'], p2p_corr_gt, return_mean=False\n",
    "        )\n",
    "    geo_err_est = geodist_metric.calculate_geodesic_error(\n",
    "        dist_x, data['first']['corr'], data['second']['corr'], p2p_est, return_mean=False\n",
    "        )\n",
    "    geo_err_est_zo = geodist_metric.calculate_geodesic_error(\n",
    "        dist_x, data['first']['corr'], data['second']['corr'], p2p_est_zo, return_mean=False\n",
    "        )\n",
    "    \n",
    "    mse_fmap = torch.sum((C_gt_xy_corr[:num_evecs,:num_evecs] - Cxy_est) ** 2)\n",
    "    mse_abs_fmap = torch.sum((C_gt_xy_corr[:num_evecs,:num_evecs].abs() - Cxy_est.abs()) ** 2)\n",
    "    \n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 8, figsize=(20, 3))\n",
    "    \n",
    "    l = 0\n",
    "    h = 32\n",
    "\n",
    "    plotting_utils.plot_Cxy(fig, axs[0], C_xy_est,\n",
    "                            f'Pred, {geo_err_est.mean() * 100:.2f}', l, h, show_grid=False, show_colorbar=False)\n",
    "    plotting_utils.plot_Cxy(fig, axs[1], C_gt_xy_corr,\n",
    "                            f'GT corrected, {geo_err_corr_gt.mean() * 100:.2f}', l, h, show_grid=False, show_colorbar=False)\n",
    "    plotting_utils.plot_Cxy(fig, axs[2], C_gt_xy,\n",
    "                            f'GT orig, {geo_err_gt.mean() * 100:.2f}', l, h, show_grid=False, show_colorbar=False)\n",
    "    plotting_utils.plot_Cxy(fig, axs[3], C_xy_est - C_gt_xy_corr[:num_evecs,:num_evecs],\n",
    "                            f'Error', l, h, show_grid=False, show_colorbar=False)\n",
    "    plotting_utils.plot_Cxy(fig, axs[4], C_xy_est_zo[:num_evecs, :num_evecs],\n",
    "                            f'After ZO, {geo_err_est_zo.mean() * 100:.2f}', l, h, show_grid=False, show_colorbar=False)\n",
    "    plotting_utils.plot_Cxy(fig, axs[5], C_xy_est_zo[:num_evecs, :num_evecs] - C_gt_xy_corr[:num_evecs,:num_evecs],\n",
    "                            f'Error ZO', l, h, show_grid=False, show_colorbar=False)\n",
    "    # plotting_utils.plot_Cxy(fig, axs[4], Cxy_est.abs() - C_gt_xy_corr.abs(),\n",
    "    #                         f'Error abs', l, h, show_grid=False, show_colorbar=False)\n",
    "    plotting_utils.plot_Cxy(fig, axs[6], C_xy_est_zo_first,\n",
    "                            f'C_xy_est_zo_first', l, h, show_grid=False, show_colorbar=False)\n",
    "    plotting_utils.plot_Cxy(fig, axs[7], C_xy_est_zo_second,\n",
    "                            f'C_xy_est_zo_second', l, h, show_grid=False, show_colorbar=False)\n",
    "    \n",
    "    # replace code above with writing to log file\n",
    "    # with open(log_file_name, 'a') as f:\n",
    "    #     f.write(f'{i}\\n')\n",
    "    #     f.write(f'Geo error GT: {geo_err_gt.mean() * 100:.2f}\\n')\n",
    "    #     f.write(f'Geo error GT corr: {geo_err_corr_gt.mean() * 100:.2f}\\n')\n",
    "    #     f.write(f'Geo error est: {geo_err_est.mean() * 100:.2f}\\n')\n",
    "    #     f.write(f'Geo error est zo: {geo_err_est_zo.mean() * 100:.2f}\\n')\n",
    "    #     f.write(f'MSE fmap: {mse_fmap:.3f}\\n')\n",
    "    #     f.write(f'MSE abs fmap: {mse_abs_fmap:.3f}\\n')\n",
    "    #     f.write('-----------------------------------\\n')\n",
    "    \n",
    "    # break\n",
    "    # plt.savefig(f'{fig_dir}/{i}.png')\n",
    "    # plt.close()\n",
    "    \n",
    "    # print the stats instead of writing to file\n",
    "    print(f'{i}')\n",
    "    print(f'Geo error GT: {geo_err_gt.mean() * 100:.2f}')\n",
    "    print(f'Geo error GT corr: {geo_err_corr_gt.mean() * 100:.2f}')\n",
    "    print(f'Geo error est: {geo_err_est.mean() * 100:.2f}')\n",
    "    print(f'Geo error est zo: {geo_err_est_zo.mean() * 100:.2f}')\n",
    "    print(f'MSE fmap: {mse_fmap:.3f}')\n",
    "    print(f'MSE abs fmap: {mse_abs_fmap:.3f}')\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # print(f'{i:2d}) ratio {geo_err_est.mean() / geo_err_corr_gt.mean():.2f}')\n",
    "    \n",
    "    ratio_curr = geo_err_est.mean() / geo_err_corr_gt.mean()\n",
    "    geo_err_curr = geo_err_est.mean() * 100\n",
    "    \n",
    "    ratios.append(ratio_curr)\n",
    "    geo_errs.append(geo_err_curr)\n",
    "    geo_errs_zo.append(geo_err_est_zo.mean() * 100)\n",
    "    Cxy_est_list.append(Cxy_est)\n",
    "    C_gt_xy_corr_list.append(C_gt_xy_corr)\n",
    "    \n",
    "    # data_range.set_description(\n",
    "    #     f'Geo error est: {geo_err_curr:.2f}, '+\\\n",
    "    #     f'Mean {torch.tensor(geo_errs).mean():.2f}, '+\\\n",
    "    #     f'Median {torch.tensor(geo_errs).median():.2f}, '+\\\n",
    "    #     f'Ratio: {ratio_curr:.2f}, '+\\\n",
    "    #     f'Mean: {torch.tensor(ratios).mean():.2f}, '+\\\n",
    "    #     f'Median: {torch.tensor(ratios).median():.2f}'\n",
    "    #     )\n",
    "\n",
    "\n",
    "ratios = torch.tensor(ratios)\n",
    "geo_errs = torch.tensor(geo_errs)\n",
    "geo_errs_zo = torch.tensor(geo_errs_zo)\n",
    "    \n",
    "# replace code above with writing to log file\n",
    "with open(log_file_name, 'a') as f:\n",
    "    f.write('-----------------------------------\\n')\n",
    "    f.write('Total statistics\\n')\n",
    "    f.write('-----------------------------------\\n')\n",
    "    f.write(f'Zoomout geo err mean: {geo_errs_zo.mean():.2f}\\n')\n",
    "    f.write(f'Zoomout geo err median: {geo_errs_zo.median():.2f}\\n')\n",
    "    f.write(f'Zoomout geo err min: {geo_errs_zo.min():.2f}\\n')\n",
    "    f.write(f'Zoomout geo err max: {geo_errs_zo.max():.2f}\\n')        \n",
    "    f.write('-----------------------------------\\n')\n",
    "    f.write(f'Mean geo err: {geo_errs.mean():.2f}\\n')\n",
    "    f.write(f'Median geo err: {geo_errs.median():.2f}\\n')\n",
    "    f.write(f'Min geo err: {geo_errs.min():.2f}\\n')\n",
    "    f.write(f'Max geo err: {geo_errs.max():.2f}\\n')\n",
    "    f.write('\\n')\n",
    "    f.write(f'Mean ratio: {ratios.mean():.2f}\\n')\n",
    "    f.write(f'Median ratio: {ratios.median():.2f}\\n')\n",
    "    f.write(f'Min ratio: {ratios.min():.2f}\\n')\n",
    "    f.write(f'Max ratio: {ratios.max():.2f}\\n')\n",
    "    f.write('\\n')\n",
    "    f.write('-----------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "\n",
    "C_xy_est_zo = torch.linalg.pinv(C_xy_est_zo_first) @ C_xy_est_zo_second\n",
    "\n",
    "plotting_utils.plot_Cxy(fig, axs[0], C_xy_est_zo_first,\n",
    "                        f'C_xy_est_zo_first', l, h, show_grid=False, show_colorbar=False)\n",
    "plotting_utils.plot_Cxy(fig, axs[1], torch.linalg.pinv(C_xy_est_zo_first),\n",
    "                        f'pinv', l, h, show_grid=False, show_colorbar=False)\n",
    "plotting_utils.plot_Cxy(fig, axs[2], C_xy_est_zo,\n",
    "                        f'C_xy_est_zo', l, h, show_grid=False, show_colorbar=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
