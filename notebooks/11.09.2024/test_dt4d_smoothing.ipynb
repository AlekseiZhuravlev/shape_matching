{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# models\n",
    "from my_code.models.diag_conditional import DiagConditionedUnet\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "import my_code.datasets.template_dataset as template_dataset\n",
    "\n",
    "import my_code.diffusion_training_sign_corr.data_loading as data_loading\n",
    "\n",
    "import networks.diffusion_network as diffusion_network\n",
    "import matplotlib.pyplot as plt\n",
    "import my_code.utils.plotting_utils as plotting_utils\n",
    "import utils.fmap_util as fmap_util\n",
    "import metrics.geodist_metric as geodist_metric\n",
    "from my_code.sign_canonicalization.training import predict_sign_change\n",
    "import argparse\n",
    "from pyFM_fork.pyFM.refine.zoomout import zoomout_refine\n",
    "import my_code.utils.zoomout_custom as zoomout_custom\n",
    "import my_code.sign_canonicalization.test_sign_correction as test_sign_correction\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from utils.shape_util import compute_geodesic_distmat\n",
    "from my_code.utils.median_p2p_map import get_median_p2p_map\n",
    "\n",
    "\n",
    "\n",
    "tqdm._instances.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        self.experiment_name = 'pair_5_xy_64_64_128_128'\n",
    "        self.checkpoint_name = 'epoch_99'\n",
    "        self.dataset_name = 'DT4D_intra_pair'\n",
    "        self.split = 'test'\n",
    "        self.num_iters_avg = 50\n",
    "        \n",
    "        self.smoothing_type = 'taubin'\n",
    "        self.smoothing_iter = 5\n",
    "        \n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "experiment_name = args.experiment_name\n",
    "checkpoint_name = args.checkpoint_name\n",
    "\n",
    "### config\n",
    "exp_base_folder = f'/home/s94zalek_hpc/shape_matching/my_code/experiments/ddpm/{experiment_name}'\n",
    "with open(f'{exp_base_folder}/config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "### model\n",
    "model = DiagConditionedUnet(config[\"model_params\"]).to('cuda')\n",
    "# model.load_state_dict(torch.load(f\"{exp_base_folder}/checkpoints/{checkpoint_name}\"))\n",
    "\n",
    "if \"accelerate\" in config and config[\"accelerate\"]:\n",
    "    accelerate.load_checkpoint_in_model(model, f\"{exp_base_folder}/checkpoints/{checkpoint_name}/model.safetensors\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(f\"{exp_base_folder}/checkpoints/{checkpoint_name}\"))\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "# algorithm\n",
    "# smooth the single dataset\n",
    "# for each mesh, correct the first evecs, get the conditioning\n",
    "\n",
    "# for each pair\n",
    "# sample the model with conditioning\n",
    "# zoomout using corrected evecs\n",
    "\n",
    "\n",
    "\n",
    "### Sign correction network\n",
    "sign_corr_net = diffusion_network.DiffusionNet(\n",
    "    **config[\"sign_net\"][\"net_params\"]\n",
    "    ).to('cuda')\n",
    "    \n",
    "sign_corr_net.load_state_dict(torch.load(\n",
    "        f'{config[\"sign_net\"][\"net_path\"]}/{config[\"sign_net\"][\"n_iter\"]}.pth'\n",
    "        ))\n",
    "\n",
    "### sample the model\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2',\n",
    "                                clip_sample=True)\n",
    "\n",
    "### test dataset\n",
    "dataset_name = args.dataset_name\n",
    "split = args.split\n",
    "\n",
    "single_dataset, test_dataset = data_loading.get_val_dataset(\n",
    "    dataset_name, split, 200, preload=False, return_evecs=True\n",
    "    )\n",
    "# sign_corr_net.cache_dir = single_dataset.lb_cache_dir\n",
    "\n",
    "single_dataset_remeshed = test_sign_correction.remesh_dataset(\n",
    "    dataset=single_dataset, \n",
    "    name=dataset_name,\n",
    "    remesh_targetlen=1,\n",
    "    smoothing_type=args.smoothing_type,\n",
    "    smoothing_iter=args.smoothing_iter,\n",
    "    num_evecs=200,\n",
    ")\n",
    "\n",
    "\n",
    "num_evecs = config[\"model_params\"][\"sample_size\"]\n",
    "\n",
    "##########################################\n",
    "# Logging\n",
    "##########################################\n",
    "\n",
    "log_dir = f'{exp_base_folder}/eval/{checkpoint_name}/{dataset_name}-{split}/{args.smoothing_type}-{args.smoothing_iter}'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "fig_dir = f'{log_dir}/figs'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "log_file_name = f'{log_dir}/log_smooth_{args.smoothing_type}_{args.smoothing_iter}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Single stage\n",
    "##########################################\n",
    "\n",
    "data_range = tqdm(range(len(single_dataset_remeshed)), desc='Calculating conditioning, correcting evecs')\n",
    "\n",
    "for i in data_range:\n",
    "\n",
    "    data = single_dataset_remeshed[i]\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    verts_second = data['verts'].unsqueeze(0).to(device)\n",
    "    faces_second = data['faces'].unsqueeze(0).to(device)\n",
    "    \n",
    "    evecs_second = data['evecs'][:, :num_evecs].unsqueeze(0).to(device)\n",
    "    evals_second = data['evals'][:num_evecs]\n",
    "\n",
    "    # corr_second = data['corr']\n",
    "    \n",
    "    if config[\"sign_net\"][\"with_mass\"]:\n",
    "        mass_mat_second = torch.diag_embed(\n",
    "            data['mass'].unsqueeze(0)\n",
    "            ).to(device)\n",
    "    else:\n",
    "        mass_mat_second = None\n",
    "\n",
    "    # predict the sign change\n",
    "    with torch.no_grad():\n",
    "        sign_pred_second, support_vector_norm_second, _ = predict_sign_change(\n",
    "            sign_corr_net, verts_second, faces_second, evecs_second, \n",
    "            mass_mat=mass_mat_second, input_type=sign_corr_net.input_type,\n",
    "            # mass=None, L=None, evals=None, evecs=None, gradX=None, gradY=None\n",
    "            mass=data['mass'].unsqueeze(0), L=data['L'].unsqueeze(0),\n",
    "            evals=data['evals'][:config[\"sign_net\"][\"net_params\"][\"k_eig\"]].unsqueeze(0),\n",
    "            evecs=data['evecs'][:,:config[\"sign_net\"][\"net_params\"][\"k_eig\"]].unsqueeze(0),\n",
    "            gradX=data['gradX'].unsqueeze(0), gradY=data['gradY'].unsqueeze(0)\n",
    "            )\n",
    "\n",
    "    # correct the evecs\n",
    "    evecs_second_corrected = evecs_second.cpu()[0] * torch.sign(sign_pred_second).cpu()\n",
    "    evecs_second_corrected_norm = evecs_second_corrected / torch.norm(evecs_second_corrected, dim=0, keepdim=True)\n",
    "    \n",
    "    # product with support\n",
    "    if config[\"sign_net\"][\"with_mass\"]:\n",
    "        mass_mat_second = torch.diag_embed(\n",
    "            data['mass'].unsqueeze(0)\n",
    "            ).to(device)\n",
    "        \n",
    "        evecs_cond_second = torch.nn.functional.normalize(\n",
    "            support_vector_norm_second[0].cpu().transpose(0, 1) \\\n",
    "                @ mass_mat_second[0].cpu(),\n",
    "            p=2, dim=1) \\\n",
    "                @ evecs_second_corrected_norm       \n",
    "    else:\n",
    "        evecs_cond_second = support_vector_norm_second[0].cpu().transpose(0, 1) @ evecs_second_corrected_norm\n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    # Conditioning\n",
    "    ###############################################\n",
    "\n",
    "    conditioning = torch.tensor([])\n",
    "    \n",
    "    if 'evals' in config[\"conditioning_types\"]:\n",
    "        eval = evals_second.unsqueeze(0)\n",
    "        eval = torch.diag_embed(eval)\n",
    "        conditioning = torch.cat((conditioning, eval), 0)\n",
    "    \n",
    "    if 'evals_inv' in config[\"conditioning_types\"]:\n",
    "        eval_inv = 1 / evals_second.unsqueeze(0)\n",
    "        # replace elements > 1 with 1\n",
    "        eval_inv[eval_inv > 1] = 1\n",
    "        eval_inv = torch.diag_embed(eval_inv)\n",
    "        conditioning = torch.cat((conditioning, eval_inv), 0)\n",
    "    \n",
    "    if 'evecs' in config[\"conditioning_types\"]:\n",
    "        conditioning = torch.cat((conditioning,\n",
    "                                    evecs_cond_second.unsqueeze(0)), 0)\n",
    "    \n",
    "    ###############################################\n",
    "    # Correct the original evecs\n",
    "    ###############################################\n",
    "    \n",
    "    data_orig = single_dataset[i]\n",
    "    evecs_second_orig = data_orig['evecs'][:, :num_evecs]\n",
    "    \n",
    "    prod_evecs_orig_remesh_corrected = evecs_second_orig.transpose(0, 1) @ evecs_second_corrected[data['corr_orig_to_remeshed']].cpu()\n",
    "\n",
    "    evecs_orig_signs = torch.sign(torch.diagonal(prod_evecs_orig_remesh_corrected, dim1=0, dim2=1))\n",
    "    evecs_second_corrected_orig = evecs_second_orig * evecs_orig_signs\n",
    "    \n",
    "    evecs_second_orig_zo = torch.cat(\n",
    "        [evecs_second_corrected_orig,\n",
    "            data_orig['evecs'][:, num_evecs:]], 1)\n",
    "\n",
    "    ###############################################\n",
    "    # Save the data\n",
    "    ###############################################\n",
    "\n",
    "    single_dataset.additional_data[i]['evecs_zo'] = evecs_second_orig_zo\n",
    "    single_dataset.additional_data[i]['conditioning'] = conditioning\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dataset_remeshed[i]['verts_orig'].shape, single_dataset_remeshed[i]['verts'].shape, single_dataset_remeshed[i]['corr_orig_to_remeshed'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
